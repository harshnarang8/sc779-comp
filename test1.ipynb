{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPuSBOSiBRZRFLgIK4xwrRH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshnarang8/sc779-comp/blob/main/test1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyuNBOVy64JJ"
      },
      "source": [
        "This model file is for testing how the transformerencoder and transformerencoderlayer modules in pytorch work. The code has mostly been sourced from the transformer tutorial on the pytorch website linked in the doc given in the cs779 course."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilaOQl_zrCHn"
      },
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53GJW8ZXrhBg"
      },
      "source": [
        "class TransformerModel(nn.Module):\n",
        "\n",
        "  def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
        "    super(TransformerModel, self).__init__()\n",
        "    from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "    self.model_type = 'Transformer'\n",
        "    self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
        "    encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
        "    self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "    self.encoder = nn.Embedding(ntoken, ninp)\n",
        "    self.ninp = ninp\n",
        "    self.decoder = nn.Linear(ninp, ntoken)\n",
        "\n",
        "    self.init_weights()\n",
        "\n",
        "  def generate_square_subsequent_mask(self, sz):\n",
        "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "  \n",
        "  def init_weights(self):\n",
        "    initrange = 0.1\n",
        "    self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "    self.decoder.bias.data.zero_()\n",
        "    self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "  \n",
        "  def forward(self, src, src_mask):\n",
        "    src = self.encoder(src)*math.sqrt(self.ninp)\n",
        "    src = self.pos_encoder(src)\n",
        "    output = self.transformer_encoder(src, src_mask)\n",
        "    output = self.decoder(output)\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIC1vthNvMc5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}